{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "파이토치첫걸음6-2",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM9aY0+/iNct9UFfgSb0Atj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Last-remote11/Algorithms/blob/main/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98%EC%B2%AB%EA%B1%B8%EC%9D%8C6_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6PoFmPG45WS"
      },
      "source": [
        "###원 핫 벡터로 단어를 표현하면 모든 단어들의 내적이 0이 되어 유사도를 나타낼 수 없게 된다.<br/>\n",
        "###원 핫 벡터 대신 임베딩은 간단히 말해 단어의 의미를 벡터화하는 것.<br/>\n",
        "###단어의 유사성이 높으면 내적했을 때 값이 비교적 크게 될듯?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o0JYI2AyF6c"
      },
      "source": [
        "## 모듈 import, 파일 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AJ5YLywuQcw"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "import time, math\n",
        "import unidecode\n",
        "\n",
        "!rm -r data\n",
        "import os \n",
        "\n",
        "# 경로 생성과 데이터 다운로드\n",
        "try:\n",
        "  os.mkdir(\"./data\")\n",
        "except:\n",
        "  pass\n",
        "!wget https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt -P ./data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO7qoiByxion",
        "outputId": "fd921724-f666-490a-846f-c0e4ad019c6f"
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.2.0-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 21.5 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 40 kB 32.3 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 28.2 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 26.8 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 81 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 92 kB 26.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 122 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 133 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 153 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 163 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 174 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 184 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 194 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 204 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 215 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 225 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 235 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 241 kB 26.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BywJzesxDzs",
        "outputId": "f43ab3ee-723a-4d8a-bd31-d647c759da6b"
      },
      "source": [
        "all_chars = string.printable\n",
        "n_chars = len(all_chars)\n",
        "print(all_chars)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
            "\r\u000b\f\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC9WlSiXyMFC"
      },
      "source": [
        "## 하이퍼파라미터 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAvY-gEix4vw"
      },
      "source": [
        "num_epochs = 2000\n",
        "print_every = 100\n",
        "plot_every = 10\n",
        "\n",
        "chunk_len = 200\n",
        "\n",
        "hidden_size = 100\n",
        "batch_size = 1\n",
        "num_layers = 1\n",
        "embedding_size = 70\n",
        "lr = 0.002"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlxZEGeayU7b",
        "outputId": "c5ee9812-e31d-4942-e11b-8be681641a3a"
      },
      "source": [
        "# 셰익스피어 예제\n",
        "file = unidecode.unidecode(open('./data/input.txt').read())\n",
        "file_len = len(file)\n",
        "print(file_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEIInTxxzMKg"
      },
      "source": [
        "모든 파일을 한꺼번에 학습할 수 없다.<br/>\n",
        "그래서 일정한 크기로 잘라주는데 이것을 앞서 정의했던 chunk 라고 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vb8aj7dzJIg"
      },
      "source": [
        "# 랜덤한 위치에서 랜덤한 길이의 문자들을 불러옴\n",
        "def random_chunk():\n",
        "    # 랜덤한 시작index와 랜덤한 끝index\n",
        "    start_index = random.randint(0, file_len - chunk_len) \n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file[start_index:end_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2EciHGz3H9b",
        "outputId": "b6b059d8-3a75-49c5-88d6-3d2a2c006cc7"
      },
      "source": [
        "# 문자들을 앞서 정의했던 문자열 리스트(all_chars)에 해당하는 인덱스로 이루어진 텐서로 변환해주는 함수\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = all_chars.index(string[c])\n",
        "    return tensor\n",
        "    \n",
        "print(char_tensor('shiranui_flare'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([28, 17, 18, 27, 10, 23, 30, 18, 88, 15, 21, 10, 27, 14])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSH8fNck3_mf"
      },
      "source": [
        "# 앞서 정의한 두 함수를 이용하여 랜덤한 훈련 세트를 만들어주는 함수\n",
        "def random_train_set():\n",
        "    chunk = random_chunk()\n",
        "    inp = char_tensor(chunk[:-1])\n",
        "    target = char_tensor(chunk[1:])\n",
        "    return inp, target # 입력과 목표"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABAKaDgd4jI-"
      },
      "source": [
        "# RNN 클래스를 만들고 인스턴스화\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # 임베딩을 70차원으로 한다 하더라도 출력에서는 다시 문자 개수만큼의 벡터로 변환해줘야한다.\n",
        "        # 그 역할을 디코더가 담당\n",
        "        self.encoder = nn.Embedding(self.input_size, self.embedding_size)\n",
        "        self.rnn = nn.RNN(self.embedding_size, self.hidden_size, self.num_layers)\n",
        "        self.decoder = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        out = self.encoder(input.view(1,-1))\n",
        "        out,hidden = self.rnn(out,hidden)\n",
        "        out = self.decoder(out.view(batch_size,-1))\n",
        "        return out,hidden\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        return hidden\n",
        "\n",
        "model = RNN(n_chars, embedding_size, hidden_size, n_chars, num_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP8E__VD8FBp"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8Ak5jsV9T7s"
      },
      "source": [
        "def test():\n",
        "    start_str = 'b'\n",
        "    inp = char_tensor(start_str)\n",
        "    hidden = model.init_hidden()\n",
        "    x = inp\n",
        "\n",
        "    print(start_str,end=\"\")\n",
        "\n",
        "    for i in range(200):\n",
        "        output, hidden = model(x, hidden)\n",
        "\n",
        "        output_dist = output.data.view(-1).div(0.8).exp() # 다음에 나올 문자를 확률로 표현한 행렬\n",
        "        top_i = torch.multinomial(output_dist, 1)[0] \n",
        "        # 행렬의 max를 쓰지 않는 이유는 the the the 같이 제일 높은 확률의 문자만 반복되기 때문이다\n",
        "        predicted_char = all_chars[top_i]\n",
        "\n",
        "        # 예측한 문자를 출력하고 다음 입력값을 이번 출력값으로 지정\n",
        "        print(predicted_char,end=\"\")\n",
        "        x = char_tensor(predicted_char)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knouUDbZ-hjl"
      },
      "source": [
        "## 모델 훈련"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZI8z7h7-cck",
        "outputId": "55739464-7d73-4173-994c-89308dde0118"
      },
      "source": [
        "for i in range(num_epochs):\n",
        "    inp, label = random_train_set()\n",
        "    hidden = model.init_hidden()\n",
        "\n",
        "    loss = torch.tensor([0]).type(torch.FloatTensor)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for j in range(chunk_len - 1):\n",
        "        x = inp[j]\n",
        "        y_ = label[j].unsqueeze(0).type(torch.LongTensor) # 정답\n",
        "        y, hidden = model(x, hidden)\n",
        "        loss += loss_func(y, y_) # i번째 에포크의 chunk의 전체 손실을 기록\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(\"\\n\",loss/chunk_len,\"\\n\")\n",
        "        test()\n",
        "        print(\"\\n\",\"=\"*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " tensor([4.5690], grad_fn=<DivBackward0>) \n",
            "\n",
            "b!NvAXGO[;An?M`}~FtvJ4i*h9/A a[[:'Q|$7&{(Sm\rTNA%}2T[V(an\n",
            "\".4ndA|7d\\q]\\9>jn$S}KMe*e9^xL]ckj\rgbXl<T*H:IykV\"[_A~v~Gp+.\"prZi-{wz_w&\tgAj=*o' h1.EZq=F8v*JY*'u*\",[\fN%E-r'C&cP@\n",
            "w>';riEbK\\j`$gc$1ke7s(*\n",
            "}*>oM\u000b8\\\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.8615], grad_fn=<DivBackward0>) \n",
            "\n",
            "bopu thenet hThe the Wand Ihen ar rrand mes rire?sl be cod Iou te thes hith ses sus shy thd thate I.rg,\n",
            "\n",
            "emt harl the tou sor leu\n",
            "Cif tinr sfthes sit!\n",
            "W hes, thenr.\n",
            "\n",
            "Hou kerges art or sppor younl\n",
            "\n",
            "Acpr\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.2722], grad_fn=<DivBackward0>) \n",
            "\n",
            "bength thy meat I hemld pour goll toit deret too te my theer the my, and rave, homas the saash tht muran to three my the the cod set in the, what it happ ay the the in in tht, in hif at siy ind my thin\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.2751], grad_fn=<DivBackward0>) \n",
            "\n",
            "beaw ve that be then,\n",
            "Thate me thencarestongworn ko sertors ufours and porthy the his why the in With wore and me the come not is shan be mangh hhe the her and oen ther for beat?\n",
            "\n",
            "LUCERETS:\n",
            "IE:\n",
            "O stain\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.1490], grad_fn=<DivBackward0>) \n",
            "\n",
            "boot to this meef mos lones\n",
            "Los nothern thout, thim so; there ther to it shourd nat lay:\n",
            "Dings hare youn wistind, sine,\n",
            "Io feen he wither, tolann and we bant the on.\n",
            "\n",
            "BOROLO:\n",
            "Sard, hare mid\n",
            "On ay thtu \n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.1315], grad_fn=<DivBackward0>) \n",
            "\n",
            "ble br\u000bat gon, wees the cren. I were and faat I tull on whall xore;\n",
            "\n",
            "BOLI. RING\n",
            "DUCINBIO:\n",
            "We of thes be comerale hit mithse you stuur.\n",
            "\n",
            "DUCENTIU:\n",
            "The alling am bonG not or't thas lay, dy wits came:\n",
            "Mab\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.0943], grad_fn=<DivBackward0>) \n",
            "\n",
            "blonges-\n",
            "ABYld de way thour what sance,\n",
            "Doun's memy sin plownoll, forifho, lenter mur dea bearnern, I mid teet this aganters.\n",
            "\n",
            "CANEESTE:\n",
            "But is thior thy so my you with shor age gut soum he my's ffruve\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([1.8782], grad_fn=<DivBackward0>) \n",
            "\n",
            "bo! me my lord, a moust then geed that And marn is you my, you grove and fut an is an nos an in mut with the kente, not of with rebain he ming Lealo be wing thy beath we the sond thing to and to ther, \n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.0374], grad_fn=<DivBackward0>) \n",
            "\n",
            "by 'Dmons under.\n",
            "\n",
            "Wepan shat andive her hiw litint\n",
            "I trows mand.\n",
            "\n",
            "COMIUT:\n",
            "AR but you to dour will well pid sher cead coul same laid Jimer's of this as susher an opt why, hay to char pay.\n",
            "\n",
            "DONIOST:\n",
            "Cowh\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.0250], grad_fn=<DivBackward0>) \n",
            "\n",
            "brownat this so come the the how, the koeghty shall speick be gaverem, gook.\n",
            "\n",
            "POLZAMTIO:\n",
            "I stris.\n",
            "\n",
            "CORIO:\n",
            "Sell wark not cere shelastantter the my tol, the seeping, that your gord\n",
            "Your as on to shut shi\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.0795], grad_fn=<DivBackward0>) \n",
            "\n",
            "bed with the lisend this as be to but that not\n",
            "I'll forlift, his is hif I waif alfor, An:\n",
            "You ret key my haster; wrilf stourd, whe kind as have to de worst you to knot the that of dy moinst fom, sint:\n",
            "\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([1.9399], grad_fn=<DivBackward0>) \n",
            "\n",
            "by foll as of wis triess wing I concems and us reareg him thous, shas we make back with fatblefendiss,\n",
            "Bote; hen IT:\n",
            "They ie you it with my hear he stere!\n",
            "\n",
            "KING BORISABER:\n",
            "Whe earn fir,\n",
            "I with is is wi\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.0078], grad_fn=<DivBackward0>) \n",
            "\n",
            "boesent a pretse; for and desire contens,\n",
            "I deed thet.\n",
            "And cor proth clove the it him ar on cury but lid! both agust a hear remester, me byeish betherepard vill\n",
            "The will ite well thou mad the deep.\n",
            "\n",
            "DU\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([1.8412], grad_fn=<DivBackward0>) \n",
            "\n",
            "blear of bist it the sor perine your cet's prier, brengs the raad with the man then they the land chin part levent him this love,\n",
            "Haje\n",
            "The as\n",
            "This have seids and suct ffor bast thouging the plest his m\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([1.8504], grad_fn=<DivBackward0>) \n",
            "\n",
            "brouner,\n",
            "As mow stail the more boinges of thy hall and of with congle of for jems me vare mare sestone and hand y thead 'tsour 'ton it this my and me plave did;\n",
            "But and to quence.\n",
            "\n",
            "OLIZAM:\n",
            "I wains shal\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([1.7802], grad_fn=<DivBackward0>) \n",
            "\n",
            "but's reld Gentle\n",
            "Conournent, you soust lother me the staw will here for of then in\n",
            "cate all arpantly for his my lork 'til he, it and lenty,\n",
            "An the:\n",
            "All ly anders:\n",
            "As make be the hann tion thee these w\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([1.6802], grad_fn=<DivBackward0>) \n",
            "\n",
            "bryour the gades and at the heads to heart'd and these bint, Iire brabour not fropreagit.\n",
            "\n",
            "FRORK:\n",
            "Ay no him\n",
            "There?\n",
            "\n",
            "GLOUG:\n",
            "Me dost loves for offard, and hee frements the beight the levil the rainst Sha\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([1.8972], grad_fn=<DivBackward0>) \n",
            "\n",
            "bet me not the brom now benson my love.\n",
            "\n",
            "Ferver; cheed: toy to mand conove the do,\n",
            "Fird that that take or shall a his one somes the his wore, the abaster that mornives be crighar rieperted all both thi\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([1.8604], grad_fn=<DivBackward0>) \n",
            "\n",
            "blure it Just, he hear on it Ackent.\n",
            "\n",
            "DUKE VIUCENTER:\n",
            "Of then, grusmeling a dood in you, serding.\n",
            "\n",
            "BERMIO:\n",
            "And aghirgets tound rave that I prace, and a crientiting.\n",
            "\n",
            "SICHENCHARI\n",
            "OLING:\n",
            "Whom this the I \n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.0311], grad_fn=<DivBackward0>) \n",
            "\n",
            "brong?\n",
            "\n",
            "KING OFERLER:\n",
            "A senst shat my cong illinourt her that shall the laid the hould priends, my manter of axtoent;\n",
            "But am that you can's or sure to I weet moremings,\n",
            "And all soldonerbent sour out in\n",
            " ====================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}